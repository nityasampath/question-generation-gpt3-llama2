# -*- coding: utf-8 -*-
"""nlu_final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-DAeFC4lzvzcHZKIF12vhiPLp0LbDHEH

##Set up

Import all the necessary packages, set up and configure llms. Set GPT-3 as default and switch to Llama 2 when necessary
"""

try:
    # This library is our indicator that the required installs
    # need to be done.
    import datasets
    root_path = '.'
except ModuleNotFoundError:
    !pip install -r requirements.txt
    root_path = 'dspy'

from datasets import load_dataset
import openai
import os
import dspy
from dotenv import load_dotenv

# keep the API keys in a `.env` file in the local root directory
load_dotenv()

os.environ["DSP_NOTEBOOK_CACHEDIR"] = os.path.join(root_path, 'cache')

openai_key = os.getenv('OPENAI_API_KEY')  # or replace with your API key (optional)

anyscale_key = os.getenv('ANYSCALE_API_KEY')  # or replace with your API key (optional)

anyscale_base = os.getenv('ANYSCALE_API_BASE')

gpt_3_turbo = dspy.OpenAI(model='gpt-3.5-turbo', api_key=openai_key)

llama_2 = dspy.Anyscale(model='meta-llama/Llama-2-70b-chat-hf', api_key=anyscale_key, api_base=anyscale_base)

dspy.configure(lm=gpt_3_turbo)

"""# SQuAD

import SQuAD dataset. Will be using the training set for few-shot examples and the validation set for testing
"""

squad = load_dataset("squad")

def get_squad_split(squad, split="validation"):
    """
    Use `split='train'` for the train split.

    Returns
    -------
    list of dspy.Example with attributes question, answer

    """
    data = zip(*[squad[split][field] for field in squad[split].features])
    exs = [dspy.Example(context=context, answer=a['text'][0], question=q).with_inputs("context", "answer")
           for eid, title, context, q, a in data]
    return exs

squad_train = get_squad_split(squad, split="train")

squad_dev = get_squad_split(squad)

squad_train[0]

squad_dev[200]

"""# Testing GPT-3 and Llama 2

Run some initial tests to make sure both models are working and can output
"""

gpt_3_turbo("Which award did Gary Zukav's first book receive?")

llama_2("Which award did Gary Zukav's first book receive?")

"""# Basic Signature and Module

Set up the question generation signature that prompts the LLM to generate the question asked from the given context and the answer. Set up the module for the question generation model, using the signature.
"""

class BasicQGSignature(dspy.Signature):
    __doc__ = """Given the context and an answer, provide a single question that can be answered by the answer based on the context."""

    context = dspy.InputField(desc="will contain answer")
    answer = dspy.InputField()
    question = dspy.OutputField(desc="short question")

class BasicQG(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_question = dspy.Predict(BasicQGSignature)

    def forward(self, context, answer):
        return self.generate_question(context=context, answer=answer)

"""# Testing question generation

Run test of the models to make sure question generation if working as expected.
"""

basic_qg_model = BasicQG()

test_example = squad_dev[200]
test_context = test_example.context
test_answer = test_example.answer
test_question = test_example.question

basic_qg_model(context=test_context, answer=test_answer)

with dspy.context(lm=llama_2):
  response = basic_qg_model(context=test_context, answer=test_answer)
response

llama_2.inspect_history(n=1)

"""# Testing few-shot prompting"""

from dspy.teleprompt import LabeledFewShot

fewshot_teleprompter = LabeledFewShot(k=3)

fewshot_qg_model = fewshot_teleprompter.compile(basic_qg_model, trainset=squad_train)

fewshot_qg_model(context=test_context, answer=test_answer)

with dspy.context(lm=llama_2):
  response = fewshot_qg_model(context=test_context, answer=test_answer)
response

gpt_3_turbo.inspect_history(n=1)

llama_2.inspect_history(n=1)

"""# Running experiments

Models are set up. Run experiments on the dev set, using the train set as the few shot examples. Run evaluation on the responses using BLUE metrics, which test similarity of the generated questions to the questions given in the dataset.
"""

import random
import tqdm
import time
import pandas as pd

len(squad_dev)

random.seed(0)
test_examples = random.sample(squad_dev, 400)
len(test_examples)

# store results as a csv
# context, answer, gold question, gpt-3, llama2
contexts = []
answers = []
gold_questions = []

for example in test_examples:
  contexts.append(example.context)
  answers.append(example.answer)
  gold_questions.append(example.question)

examples_dict = {"context" : contexts,
                 "answer" : answers,
                 "gold_question" : gold_questions}

examples_df = pd.DataFrame.from_dict(examples_dict)
examples_df.head()

# necessary experiments to run
# GPT-3 zero-shot (baseline) -> gpt3_zero_shot
# GPT-3 few-shot -> gpt3_few_shot
# Llama2 zero-shot -> llama2_zero_shot
# llama2 few-shot -> llama2_few_shot

examples_df.head()

# baseline model
gpt3_zero_shot = []
for example in tqdm.tqdm(test_examples):
    generated_question = basic_qg_model(context=example.context, answer=example.answer).question
    gpt3_zero_shot.append(generated_question)
    time.sleep(5)

if len(gpt3_zero_shot) < len(examples_df):
  gpt3_zero_shot += [''] * (len(examples_df) - len(gpt3_zero_shot))

examples_df["gpt3_zero_shot"] = gpt3_zero_shot
examples_df.head()

examples_df.to_csv('examples_df_baseline.csv', index=False)

# GPT-3 few shot
gpt3_few_shot = []
for example in tqdm.tqdm(test_examples):
    generated_question = fewshot_qg_model(context=example.context, answer=example.answer).question
    gpt3_few_shot.append(generated_question)
    time.sleep(5)

if len(gpt3_few_shot) < len(examples_df):
  gpt3_few_shot += [''] * (len(examples_df) - len(gpt3_few_shot))

examples_df["gpt3_few_shot"] = gpt3_few_shot
examples_df.head()

examples_df.to_csv('examples_df_baseline_gpt3_few_shot.csv', index=False)

# Llama2 zero shot
llama2_zero_shot = []
with dspy.context(lm=llama_2):
  for example in tqdm.tqdm(test_examples):
      generated_question = basic_qg_model(context=example.context, answer=example.answer).question
      llama2_zero_shot.append(generated_question)
      time.sleep(5)

if len(llama2_zero_shot) < len(examples_df):
  llama2_zero_shot += [''] * (len(examples_df) - len(llama2_zero_shot))

examples_df["llama2_zero_shot"] = llama2_zero_shot
examples_df.head()

examples_df.to_csv('examples_df_baseline_gpt3_few_shot_llama2.csv', index=False)

# Llama2 few shot
llama2_few_shot = []
with dspy.context(lm=llama_2):
  for example in tqdm.tqdm(test_examples):
      generated_question = fewshot_qg_model(context=example.context, answer=example.answer).question
      llama2_few_shot.append(generated_question)
      time.sleep(5)

if len(llama2_few_shot) < len(examples_df):
  llama2_few_shot += [''] * (len(examples_df) - len(llama2_few_shot))

examples_df["llama2_few_shot"] = llama2_few_shot
examples_df.head()

examples_df.to_csv('examples_df.csv', index=False)

"""# Evaluation

Quantitative evaluation: Use BLEU to evaluate the results similarity to the gold answers. Run sentence BLEU for each sentence (in comparison with gold sentence) and run corpus BLEU for entire corpus.

Qualitative evaluation: examine the generated questions and look at question types and see if there were patterns in which kinds of questions the models answered correctly. Llama2 returned a lot of extra fluff in its answers. Examine how often it did that and if it was more common in the zero-shot and few-shot.
"""

import nltk
nltk.download('punkt')

from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import corpus_bleu
from nltk.tokenize import word_tokenize
import string

# read results file as csv
examples_df = pd.read_csv('examples_df.csv')
examples_df.head(5)

# add new columns containing the sentence bleu scores for each result
# gpt3_zero_shot_bleu
# gpt3_few_shot_bleu
# llama2_zero_shot_bleu
# llama2_few_shot_bleu

exclude = set(string.punctuation)

def tokenize(sent):
  sent_stripped = ''.join(ch for ch in sent if ch not in exclude)
  return (word_tokenize(sent_stripped))

ind = 0
test_reference = tokenize(examples_df["gold_question"].iloc[ind])
test_sent = tokenize(examples_df["gpt3_zero_shot"].iloc[ind])
print(test_reference)
print(test_sent)

score = sentence_bleu([test_reference], test_sent)
print(score)

gpt3_zero_shot_bleu	= []
gpt3_few_shot_bleu = []
llama2_zero_shot_bleu = []
llama2_few_shot_bleu = []
corpus_references = []

for i in range(len(examples_df)):
  gold = examples_df["gold_question"].iloc[i]
  gpt3_zero_shot_sent = examples_df["gpt3_zero_shot"].iloc[i]
  gpt3_few_shot_sent = examples_df["gpt3_few_shot"].iloc[i]
  llama2_zero_shot_sent = examples_df["llama2_zero_shot"].iloc[i]
  llama2_few_shot_sent = examples_df["llama2_few_shot"].iloc[i]

  gpt3_zero_shot_bleu.append(sentence_bleu([gold], gpt3_zero_shot_sent))
  gpt3_few_shot_bleu.append(sentence_bleu([gold], gpt3_few_shot_sent))
  llama2_zero_shot_bleu.append(sentence_bleu([gold], llama2_zero_shot_sent))
  llama2_few_shot_bleu.append(sentence_bleu([gold], llama2_few_shot_sent))

  corpus_references.append([gold])

gpt3_zero_shot_bleu[0:5]

examples_df["gpt3_zero_shot_bleu"] = gpt3_zero_shot_bleu
examples_df["gpt3_few_shot_bleu"] = gpt3_few_shot_bleu
examples_df["llama2_zero_shot_bleu"] = llama2_zero_shot_bleu
examples_df["llama2_few_shot_bleu"] = llama2_few_shot_bleu

cols = list(examples_df.columns.values)
cols

cols_list = ['context',
 'answer',
 'gold_question',
 'gpt3_zero_shot',
 'gpt3_zero_shot_bleu',
 'gpt3_few_shot',
 'gpt3_few_shot_bleu',
 'llama2_zero_shot',
 'llama2_zero_shot_bleu',
 'llama2_few_shot',
 'llama2_few_shot_bleu']

examples_df = examples_df[cols_list]
examples_df

corpus_references[0:5]

len(corpus_references)

corpus_scores = []

corpus_scores.append(corpus_bleu(corpus_references, list(examples_df["gpt3_zero_shot"])))
corpus_scores.append(corpus_bleu(corpus_references, list(examples_df["gpt3_few_shot"])))
corpus_scores.append(corpus_bleu(corpus_references, list(examples_df["llama2_zero_shot"])))
corpus_scores.append(corpus_bleu(corpus_references, list(examples_df["llama2_few_shot"])))

corpus_scores

models_list = ["gpt3_zero_shot", "gpt3_few_shot", "llama2_zero_shot", "llama2_few_shot"]

corpus_dict = {"model" : models_list, "corpus_BLEU_score" : corpus_scores}

corpus_df = pd.DataFrame.from_dict(corpus_dict)
corpus_df

examples_df.to_csv('examples_df_BLEU.csv', index=False)

corpus_df.to_csv('corpus_BLEU.csv', index=False)

# read results file as csv
examples_df = pd.read_csv('examples_df_BLEU.csv')
examples_df.head(5)

gpt3_zero_shot_df = examples_df[["context", "answer", "gold_question", "gpt3_zero_shot", "gpt3_zero_shot_bleu"]]
gpt3_zero_shot_df.head(5)

gpt3_zero_shot_df.to_csv('gpt3_zero_shot_df_BLEU.csv', index=False)

gpt3_few_shot_df = examples_df[["context", "answer", "gold_question", "gpt3_few_shot", "gpt3_few_shot_bleu"]]
gpt3_few_shot_df.head(5)

gpt3_few_shot_df.to_csv('gpt3_few_shot_df_BLEU.csv', index=False)

llama2_zero_shot_df = examples_df[["context", "answer", "gold_question", "llama2_zero_shot", "llama2_zero_shot_bleu"]]
llama2_zero_shot_df.head(5)

llama2_zero_shot_df.to_csv('llama2_zero_shot_df_BLEU.csv', index=False)

llama2_few_shot_df = examples_df[["context", "answer", "gold_question", "llama2_few_shot", "llama2_few_shot_bleu"]]
llama2_few_shot_df.head(5)

llama2_few_shot_df.to_csv('llama2_few_shot_df_BLEU.csv', index=False)